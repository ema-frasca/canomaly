{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from ast import literal_eval\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "display(HTML(\"<style>\"\n",
    "    + \"#notebook { padding-top:0px; } \" \"\"\n",
    "    + \".container { width:100%; } \"\n",
    "    + \".end_space { min-height:0px; } \"\n",
    "    + \"</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logs_path = 'C:\\\\Users\\\\emace\\\\AImageLab\\\\SRV-Continual\\\\results\\\\canomaly\\\\logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for environment in glob(logs_path + '/**/*.pyd', recursive=True):\n",
    "    with open(environment, 'r') as f:\n",
    "        experiments = [literal_eval(line) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_exp_info(exp: dict):\n",
    "    print({k: exp[k] for k in exp if k not in ['logs', 'results', 'knowledge']})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# usage example:  show_exp_images(experiments[0], True)\n",
    "def show_exp_images(exp: dict, show_origins=False):\n",
    "    for task in exp['results']:\n",
    "        cur_images = exp['results'][task]['images']\n",
    "        fig, axs = plt.subplots(2, 5, figsize=(15, 8))\n",
    "        fig.suptitle(f'TASK {task}', fontsize=30)\n",
    "        for r, row in enumerate(axs):\n",
    "            for c, cell in enumerate(row):\n",
    "                idx = r*5 + c\n",
    "                image = np.zeros((28, 28, 3), dtype=float)\n",
    "                cell.set_title(cur_images[idx]['label'])\n",
    "                orig = np.array(cur_images[idx]['original'][0])\n",
    "                recon = np.array(cur_images[idx]['reconstruction'][0]).clip(0, 1)\n",
    "                if show_origins:\n",
    "                    image[:,:,1] = orig\n",
    "                image[:,:,0] = recon\n",
    "                image[:,:,2] = recon\n",
    "                cell.imshow(image)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_weighted_auc(anomalies: np.array, scores: np.array):\n",
    "    n_anomalies = anomalies.sum().item()\n",
    "    n_normals = len(anomalies) - n_anomalies\n",
    "    weights = np.zeros_like(scores)\n",
    "    weights[anomalies == 0] = n_anomalies/len(anomalies)\n",
    "    weights[anomalies == 1] = n_normals/len(anomalies)\n",
    "    return roc_auc_score(anomalies, scores, sample_weight=weights)\n",
    "\n",
    "\n",
    "def compute_all_aucs(anomalies, scores):\n",
    "    total_auc = roc_auc_score(anomalies, scores)\n",
    "\n",
    "    n_anomalies = anomalies.sum().item()\n",
    "    n_normals = len(anomalies) - n_anomalies\n",
    "    weighted_auc = compute_weighted_auc(anomalies, scores)\n",
    "\n",
    "    min_label = 1 if n_anomalies < n_normals else 0\n",
    "    max_label = 1 - min_label\n",
    "    n_per_class = n_anomalies if n_anomalies < n_normals else n_normals\n",
    "    idxs_norm = np.where(anomalies==min_label)[0]\n",
    "    idxs_anom = np.random.choice(np.where(anomalies==max_label)[0], size=n_per_class, replace=False)\n",
    "    idxs = np.concatenate((idxs_norm, idxs_anom))\n",
    "    balanced_auc = roc_auc_score(anomalies[idxs], scores[idxs])\n",
    "\n",
    "    return total_auc, weighted_auc, balanced_auc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_exp_metrics(exp: dict, per_task=True):\n",
    "    knowledge = []\n",
    "    metrics = pd.DataFrame(index=exp['results'], columns=[str(labels) for labels in exp['knowledge'].values()] + ['total'], dtype='float')\n",
    "    for t, task in enumerate(exp['results']):\n",
    "        knowledge.extend(exp['knowledge'][task])\n",
    "        targets = np.array(exp['results'][task]['targets'])\n",
    "        scores = np.array(exp['results'][task]['rec_errs'])\n",
    "        anomalies = (~np.isin(targets, knowledge)).astype(int)\n",
    "        auc = compute_weighted_auc(anomalies, scores)\n",
    "        metrics.loc[task, 'total'] = auc\n",
    "        # print(f'task {task}: {auc}')\n",
    "\n",
    "        if t > 0 and per_task:\n",
    "            for in_t, in_task in zip(range(t+1), exp['results']):\n",
    "                np_knowledge = np.array(knowledge)\n",
    "                excluded_labels = np_knowledge[~np.isin(np_knowledge, exp['knowledge'][in_task])].tolist()\n",
    "                mask = ~np.isin(targets, excluded_labels)\n",
    "                in_targets = targets[mask]\n",
    "                in_scores = scores[mask]\n",
    "                in_anomalies = (~np.isin(in_targets, knowledge)).astype(int)\n",
    "                in_auc = compute_weighted_auc(in_anomalies, in_scores)\n",
    "                metrics.loc[task, str(exp['knowledge'][in_task])] = in_auc\n",
    "                # print(f'  t{in_task} vs all: {in_auc}')\n",
    "        else:\n",
    "            metrics.loc[task, str(exp['knowledge'][task])] = auc\n",
    "\n",
    "    final_auc = metrics.loc[task, 'total']\n",
    "    average_auc = metrics.loc[:, \"total\"].mean()\n",
    "    # print(f'final {final_auc} average {average_auc}')\n",
    "    return final_auc, average_auc, metrics\n",
    "\n",
    "def show_aucs_per_task(task_aucs: pd.DataFrame):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    sns.heatmap(task_aucs, annot=True, ax=ax, cmap='Reds', cbar=False)\n",
    "    plt.ylabel('Task')\n",
    "    plt.xlabel('Class')\n",
    "    plt.title('Auc per task and class')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def reconstruction_confusion_matrix(exp: dict):\n",
    "    labels = np.unique(np.array(next(iter(exp['results'].values()))['targets'])).tolist()\n",
    "    indexes = [key + str(exp['knowledge'][key]) for key in exp['knowledge']]\n",
    "    matrix = pd.DataFrame(index=indexes,\n",
    "                          columns=labels, dtype='float')\n",
    "\n",
    "    for idx, task in zip(indexes, exp['results']):\n",
    "        scores = np.array(exp['results'][task]['rec_errs'])\n",
    "        targets = np.array(exp['results'][task]['targets'])\n",
    "        for label in labels:\n",
    "            matrix.loc[idx, label] = scores[targets == label].mean()\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def show_conf_matrix(cmatrix: pd.DataFrame):\n",
    "    fig,ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(data=cmatrix, ax=ax,annot=True, cbar=False, cmap='Reds')\n",
    "    plt.ylabel('Task')\n",
    "    plt.xlabel('Class')\n",
    "    plt.title('Reconstruction error per task and class')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## print metrics of experiments\n",
    "\n",
    "exp = experiments[0]\n",
    "\n",
    "print_exp_info(exp)\n",
    "\n",
    "# show_exp_images(exp, False)\n",
    "\n",
    "# final_auc, average_auc, task_aucs =  compute_exp_metrics(exp)\n",
    "# print(f'final {final_auc} average {average_auc}')\n",
    "# show_aucs_per_task(task_aucs)\n",
    "\n",
    "cmatrix = reconstruction_confusion_matrix(exp)\n",
    "show_conf_matrix(cmatrix)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# task_aucs.to_dict(orient='index')\n",
    "# pd.DataFrame.from_dict(task_aucs.to_dict(orient='index'), orient='index')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}