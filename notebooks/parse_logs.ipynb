{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from ast import literal_eval\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "display(HTML(\"<style>\"\n",
    "    + \"#notebook { padding-top:0px; } \" \"\"\n",
    "    + \".container { width:100%; } \"\n",
    "    + \".end_space { min-height:0px; } \"\n",
    "    + \"</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logs_path = 'C:\\\\Users\\\\emace\\\\AImageLab\\\\SRV-Continual\\\\results\\\\canomaly\\\\logs'\n",
    "logs_ext = '.pyd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def delete_lines(content: str, filenames: list[str]):\n",
    "    for filename in filenames:\n",
    "        print(f'In file {filename}:')\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        end_lines = []\n",
    "        for i, line in enumerate(lines):\n",
    "            if content in line:\n",
    "                print(f'Deleted line {i}: {line}')\n",
    "            else:\n",
    "                end_lines.append(line)\n",
    "        if len(lines) == len(end_lines):\n",
    "            print('No line to delete')\n",
    "        else:\n",
    "            with open(filename, 'w') as f:\n",
    "                f.writelines(end_lines)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# delete_lines('443c988d-d031-4168-951f-e60ac3df1a76', glob(logs_path + '/*-fmnist*/*-2*/*' + logs_ext, recursive=True))\n",
    "# delete_lines('443c988d-d031-4168-951f-e60ac3df1a76', glob(logs_path + '/*-mnist*/*-2*/*' + logs_ext, recursive=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp_dict = {}\n",
    "exp_list = []\n",
    "for log_file in glob(logs_path + '/**/*' + logs_ext, recursive=True):\n",
    "    print(log_file)\n",
    "    with open(log_file, 'r') as f:\n",
    "        exps = []\n",
    "        for i, line in enumerate(f.readlines()):\n",
    "            try:\n",
    "                exps.append(literal_eval(line.replace('nan', 'None')))\n",
    "            except:\n",
    "                print(f'Unparsed line {i}:\\n\\t{exps[:-1]}\\n-->\\t{line}')\n",
    "        exp_list.extend(exps)\n",
    "        exps = {exp['id']: exp for exp in exps}\n",
    "        exp_dict = {**exp_dict, **exps}\n",
    "        # literal_eval(f.readline().replace('nan', 'None'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_exp_info(exp: dict):\n",
    "    print({k: exp[k] for k in exp if k not in ['logs', 'results', 'knowledge']})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# usage example:  show_exp_images(experiments[0], True)\n",
    "def show_exp_images(exp: dict, show_origins=False):\n",
    "    for task in exp['results']:\n",
    "        cur_images = exp['results'][task]['images']\n",
    "        fig, axs = plt.subplots(2, 5, figsize=(15, 8))\n",
    "        fig.suptitle(f'TASK {task} {exp[\"knowledge\"][task]}', fontsize=30)\n",
    "        for r, row in enumerate(axs):\n",
    "            for c, cell in enumerate(row):\n",
    "                idx = r*5 + c\n",
    "                image = np.zeros((28, 28, 3), dtype=float)\n",
    "                cell.set_title(cur_images[idx]['label'])\n",
    "                orig = np.array(cur_images[idx]['original'][0])\n",
    "                recon = np.array(cur_images[idx]['reconstruction'][0]).clip(0, 1)\n",
    "                if show_origins:\n",
    "                    image[:,:,1] = orig\n",
    "                image[:,:,0] = recon\n",
    "                image[:,:,2] = recon\n",
    "                cell.imshow(image)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_weighted_auc(anomalies: np.array, scores: np.array):\n",
    "    n_anomalies = anomalies.sum().item()\n",
    "    n_normals = len(anomalies) - n_anomalies\n",
    "    weights = np.zeros_like(scores)\n",
    "    weights[anomalies == 0] = n_anomalies/len(anomalies)\n",
    "    weights[anomalies == 1] = n_normals/len(anomalies)\n",
    "    return roc_auc_score(anomalies, scores, sample_weight=weights)\n",
    "\n",
    "\n",
    "def compute_all_aucs(anomalies, scores):\n",
    "    total_auc = roc_auc_score(anomalies, scores)\n",
    "\n",
    "    n_anomalies = anomalies.sum().item()\n",
    "    n_normals = len(anomalies) - n_anomalies\n",
    "    weighted_auc = compute_weighted_auc(anomalies, scores)\n",
    "\n",
    "    min_label = 1 if n_anomalies < n_normals else 0\n",
    "    max_label = 1 - min_label\n",
    "    n_per_class = n_anomalies if n_anomalies < n_normals else n_normals\n",
    "    idxs_norm = np.where(anomalies==min_label)[0]\n",
    "    idxs_anom = np.random.choice(np.where(anomalies==max_label)[0], size=n_per_class, replace=False)\n",
    "    idxs = np.concatenate((idxs_norm, idxs_anom))\n",
    "    balanced_auc = roc_auc_score(anomalies[idxs], scores[idxs])\n",
    "\n",
    "    return total_auc, weighted_auc, balanced_auc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_exp_metrics(exp: dict, per_task=True):\n",
    "    knowledge = []\n",
    "    metrics = pd.DataFrame(index=exp['results'], columns=[str(labels) for labels in exp['knowledge'].values()] + ['total'], dtype='float')\n",
    "    for t, task in enumerate(exp['results']):\n",
    "        knowledge.extend(exp['knowledge'][task])\n",
    "        targets = np.array(exp['results'][task]['targets'])\n",
    "        scores = np.array(exp['results'][task]['rec_errs'])\n",
    "        anomalies = (~np.isin(targets, knowledge)).astype(int)\n",
    "        auc = compute_weighted_auc(anomalies, scores)\n",
    "        metrics.loc[task, 'total'] = auc\n",
    "        # print(f'task {task}: {auc}')\n",
    "\n",
    "        if t > 0 and per_task:\n",
    "            for in_t, in_task in zip(range(t+1), exp['results']):\n",
    "                np_knowledge = np.array(knowledge)\n",
    "                excluded_labels = np_knowledge[~np.isin(np_knowledge, exp['knowledge'][in_task])].tolist()\n",
    "                mask = ~np.isin(targets, excluded_labels)\n",
    "                in_targets = targets[mask]\n",
    "                in_scores = scores[mask]\n",
    "                in_anomalies = (~np.isin(in_targets, knowledge)).astype(int)\n",
    "                in_auc = compute_weighted_auc(in_anomalies, in_scores)\n",
    "                metrics.loc[task, str(exp['knowledge'][in_task])] = in_auc\n",
    "                # print(f'  t{in_task} vs all: {in_auc}')\n",
    "        else:\n",
    "            metrics.loc[task, str(exp['knowledge'][task])] = auc\n",
    "\n",
    "    final_auc = metrics.loc[task, 'total']\n",
    "    average_auc = metrics.loc[:, \"total\"].mean()\n",
    "    # print(f'final {final_auc} average {average_auc}')\n",
    "    return final_auc, average_auc, metrics\n",
    "\n",
    "def show_aucs_per_task(task_aucs: pd.DataFrame):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    sns.heatmap(task_aucs, annot=True, ax=ax, cmap='Reds', cbar=False)\n",
    "    plt.ylabel('Task')\n",
    "    plt.xlabel('Class')\n",
    "    plt.title('Auc per task and class')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def reconstruction_confusion_matrix(exp: dict):\n",
    "    labels = np.unique(np.array(next(iter(exp['results'].values()))['targets'])).tolist()\n",
    "    indexes = [key + str(exp['knowledge'][key]) for key in exp['knowledge']]\n",
    "    matrix = pd.DataFrame(index=indexes,\n",
    "                          columns=labels, dtype='float')\n",
    "\n",
    "    for idx, task in zip(indexes, exp['results']):\n",
    "        scores = np.array(exp['results'][task]['rec_errs'])\n",
    "        targets = np.array(exp['results'][task]['targets'])\n",
    "        for label in labels:\n",
    "            matrix.loc[idx, label] = scores[targets == label].mean()\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def show_conf_matrix(cmatrix: pd.DataFrame):\n",
    "    fig,ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(data=cmatrix, ax=ax,annot=True, cbar=False, cmap='Reds')\n",
    "    plt.ylabel('Task')\n",
    "    plt.xlabel('Class')\n",
    "    plt.title('Reconstruction error per task and class')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## print metrics of experiments\n",
    "def exp_disclosure(exp: dict, info=True, images=False, origins=False, aucs=False, cmatrix=False):\n",
    "    if info:\n",
    "        print_exp_info(exp)\n",
    "\n",
    "    if images:\n",
    "        show_exp_images(exp, origins)\n",
    "\n",
    "    if aucs:\n",
    "        final_auc, average_auc, task_aucs =  compute_exp_metrics(exp)\n",
    "        print(f'final {final_auc} average {average_auc}')\n",
    "        show_aucs_per_task(task_aucs)\n",
    "\n",
    "    if cmatrix:\n",
    "        cmatrix = reconstruction_confusion_matrix(exp)\n",
    "        show_conf_matrix(cmatrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_disclosure(exp_dict['d7d4d5a0-c112-4c91-8b5d-6b04d396aeea'], images=True, origins=False, cmatrix=False)\n",
    "exp_disclosure(exp_dict['ce93a2f9-78f2-4586-a9cc-4263c79a803f'], images=True, origins=False, cmatrix=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# AE: joint impara un po tutto\n",
    "# exp_disclosure(exp_dict['a8b7df79-7031-4f95-ba9c-aaae316ced5e'], images=True)\n",
    "exp_disclosure(exp_dict['3dfef3e0-f68c-4194-9caa-ded9be3005e8'], images=True, origins=False, cmatrix=False)\n",
    "exp_disclosure(exp_dict['e6ba6f77-0d03-48bd-a342-5a2f9f54a240'], images=True, origins=False, cmatrix=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_disclosure(exp_dict['6ca895ec-4878-4a60-b125-3264d816ef2a'], images=True)\n",
    "exp_disclosure(exp_dict['e3c2cf61-59ca-4df8-b1d5-204a80579857'], images=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}