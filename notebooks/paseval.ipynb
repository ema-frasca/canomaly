{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ast import literal_eval\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from IPython.core.display import display, HTML\n",
    "from numpy import nan\n",
    "display(HTML(\"<style>\"\n",
    "    + \"#notebook { padding-top:0px; } \" \"\"\n",
    "    + \".container { width:100%; } \"\n",
    "    + \".end_space { min-height:0px; } \"\n",
    "    + \"</style>\"))\n",
    "# import colors\n",
    "\n",
    "\n",
    "def highlight_max(sett):\n",
    "    if sett.dtype == 'O':\n",
    "        s = sett.str.split(' ').apply(lambda x: float(x[0]) if x[0] != '-' else np.nan)\n",
    "    else:\n",
    "        s = sett\n",
    "    is_max = s == s.max()\n",
    "    \n",
    "    return ['font-weight: bold' if v else '' for v in is_max]\n",
    "\n",
    "def highlight_group(sett):\n",
    "    try:\n",
    "        if sett.dtype == 'O':\n",
    "            oboe = sett.str.split(' ').apply(lambda x: float(x[0]) if x[0] != '-' else np.nan)\n",
    "        else:\n",
    "            oboe = sett\n",
    "        is_max = oboe.to_frame().apply(lambda x: oboe[oboe.index.get_level_values(0) == x.name[0]].max() == x.item(), axis=1)\n",
    "        return ['font-weight: bold' if v else '' for v in is_max]\n",
    "    except:\n",
    "        return ['' for _ in sett]\n",
    "\n",
    "def highlight_group_min(sett):\n",
    "    try:\n",
    "        if sett.dtype == 'O':\n",
    "            oboe = sett.str.split(' ').apply(lambda x: float(x[0]) if x[0] != '-' else np.nan)\n",
    "        else:\n",
    "            oboe = sett\n",
    "        is_max = oboe.to_frame().apply(lambda x: oboe[oboe.index.get_level_values(0) == x.name[0]].min() == x.item(), axis=1)\n",
    "        return ['font-weight: bold' if v else '' for v in is_max]\n",
    "    except:\n",
    "        return ['' for _ in sett]\n",
    "\n",
    "    \n",
    "def highlight_min(sett):\n",
    "    if sett.dtype == 'O':\n",
    "        s = sett.str.split(' ').apply(lambda x: float(x[0]) if x[0] != '-' else np.nan)\n",
    "    else:\n",
    "        s = sett\n",
    "    is_max = s == s.min()\n",
    "    \n",
    "    return ['font-weight: bold' if v else '' for v in is_max]\n",
    "\n",
    "def b_g(sett, cmap='PuBu', low=0, high=0):\n",
    "    if sett.dtype == 'O':\n",
    "        s = sett.str.split(' ').apply(lambda x: float(x[0]) if x[0] != '-' else np.nan)\n",
    "    else:\n",
    "        s = sett\n",
    "    a = s\n",
    "    rng = a.max() - a.min()\n",
    "    norm = colors.Normalize(a.min() - (rng * low),\n",
    "                        a.max() + (rng * high))\n",
    "    normed = norm(a.values)\n",
    "    c = [colors.rgb2hex(x) for x in plt.cm.get_cmap(cmap)(normed)]\n",
    "    return ['background-color: %s' % color for color in c]\n",
    "\n",
    "def method_to_df(apath, setting, dataset, method):\n",
    "    last = ' '.join([setting, dataset, method])\n",
    "    with open(apath + '%s/%s/' % (setting, dataset) + method +'/logs.pyd', 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    o = pd.DataFrame.from_records([eval(x.replace('nan', 'float(\\'nan\\')')) for x in lines])\n",
    "    \n",
    "    o = o.rename(columns={'cs_size':'buffer_size'})\n",
    "    if 'buffer_size' not in o:\n",
    "        o['buffer_size'] = 0\n",
    "    if 'job_number' in o.columns:\n",
    "        del o['job_number']\n",
    "    o['method'] = method\n",
    "    return o\n",
    "\n",
    "\n",
    "req_rep = 1\n",
    "print_args= False\n",
    "display_pre = False\n",
    "howmany_pre = 1\n",
    "print_other_metrics = False\n",
    "t_distinct_cols = ['buffer_size', 'method', 'shap_weight', 'shap_all', 'n_epochs']\n",
    "ignore_cols = []\n",
    "excluded_methods = []\n",
    "included_methods = []\n",
    "excluded_datasets = []\n",
    "\n",
    "sec_respath = None\n",
    "\n",
    "# PAMI\n",
    "respath = '../storage/results/dataset-can-mnist/classes_per_task-1/logs.pyd'#'C:\\\\Users\\\\emace\\\\AImageLab\\\\SRV-Continual\\\\results\\\\mammoth\\\\results\\\\'\n",
    "sec_respath = '../storage/results/dataset-can-mnist/classes_per_task-2/logs.pyd' #'/home/mbosc/srv-nas/mammoth-master/results/'\n",
    "# third_respath = '../storage/results/dataset-can-sole/classes_per_task-1/logs.pyd'\n",
    "third_respath = None #'/home/mbosc/phd/pami/results_buzz/'\n",
    "\n",
    "respaths = [respath, sec_respath, third_respath]\n",
    "islist = lambda col: any([type(x) == list for x in col.values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!../utils/download_results.sh /nas/softechict-nas-1/rbenaglia/canomaly-data/results/dataset-can-mnist/ ../storage/results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import file\n",
    "list_total_parsed = []\n",
    "for path in respaths:\n",
    "    if path is not None:\n",
    "        with open(os.path.join(path), 'r') as obj:\n",
    "            file = obj.readlines()\n",
    "        file_parsed = map(eval, file)\n",
    "        list_total_parsed.extend(list(file_parsed))\n",
    "    \n",
    "df_file = pd.DataFrame(list_total_parsed)\n",
    "df_file['timestamp'] = pd.to_datetime(df_file['timestamp'])\n",
    "df_file = df_file[df_file['timestamp']>'2022-02-02 16:50:00']\n",
    "# df_file = df_file[df_file['timestamp']>'2022-02-05 16:50:00']\n",
    "df_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_auc = pd.concat([pd.DataFrame.from_dict(sing[1].loc[0,'auc_per_task'], orient='index')]*2)\n",
    "# total_auc.reset_index().groupby('index').mean()\n",
    "\n",
    "def calc_mean_on_matrix(X):\n",
    "    total_df = pd.concat([pd.DataFrame.from_dict(x, orient='index') for x in X])\n",
    "    return total_df.reset_index().groupby('index').mean().to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_pm_std(x):\n",
    "    return f'{x.mean().round(3)}+-{x.std().round(3)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting links to html tags\n",
    "def path_to_image_html(path):\n",
    "    return '<img src=\"'+ path + '\" width=\"60\" >'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def compare_experiments(s, column_to_compare: list):\n",
    "    is_max = pd.Series(data=False, index=s.index)\n",
    "    is_max[column] = s.loc[column] >= threshold\n",
    "    return ['background-color: yellow' if is_max.any() else '' for v in is_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elab = df_file.copy()\n",
    "\n",
    "common_keys=['dataset','classes_per_task','model','optim','lr','batch_size','n_epochs','latent_space','joint']\n",
    "part_keys = {'sae':['sparse_weight','norm_order'], 'ae':[], 'vae':['beta_kl'],'dae':['noise_mean','noise_std']}\n",
    "elaboration_cols = {\n",
    "    'auc_final':[calc_mean_pm_std, len, np.mean, np.std],\n",
    "    'auc_average':[calc_mean_pm_std, np.mean, np.std],\n",
    "    'auc_per_task': [calc_mean_on_matrix],\n",
    "    'conf_matrix_per_task': [calc_mean_on_matrix],\n",
    "    'id': [list]\n",
    "    \n",
    "}\n",
    "task_per_task_elaboration_cols = {\n",
    "    \n",
    "}\n",
    "total_list = []\n",
    "for model, df_model in df_elab.groupby('model'):\n",
    "    df_model['model'] = model\n",
    "    total_keys = common_keys+part_keys[model]\n",
    "    \n",
    "    df_single = df_model.groupby(total_keys).agg(elaboration_cols)\n",
    "        \n",
    "    total_list.append(df_single.reset_index())\n",
    "\n",
    "df_total = pd.concat(total_list).set_index(common_keys)\n",
    "df_total.columns = ['_'.join(x) for x in df_total.columns.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(matrix_dict, title=None, cmap='Reds'):\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    sns.heatmap(pd.DataFrame.from_dict(matrix_dict, orient='index'), annot=True, cbar=False, cmap=cmap, ax=ax, linewidths=0.1)\n",
    "    ax.set_xlabel('Label')\n",
    "    ax.set_ylabel('Task')\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "def plot_heatmaps(df: pd.DataFrame, which_col=('auc_per_task', 'calc_mean_on_matrix')):\n",
    "    for title,row in df.iterrows():\n",
    "        plot_heatmap(row.loc[which_col], title='\\n'.join(f'{x}: {y}' for x,y in zip(df.index.names,title)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_on_df_total(df, sort_by_list = [],ascending=False, **kwargs):\n",
    "    fil = ' and '.join([f'{k}=={v}' if isinstance(v,(int,float, bool)) else f'{k}==\"{v}\"' for k,v in kwargs.items()])\n",
    "    df_sub = df.reset_index().query(fil).set_index(common_keys)\n",
    "    return df_sub.sort_values(by=sort_by_list, ascending=ascending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kwargs = dict(joint=False,classes_per_task=2,)\n",
    "df_filter = filter_on_df_total(df_total, \n",
    "                               sort_by_list=['auc_average_mean'], \n",
    "                               ascending=False, \n",
    "                               **kwargs)\n",
    "df_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_ = df_filter.iloc[1, df_filter.columns.to_list().index('id_list')][0]\n",
    "id_ = '23b78d21-6ae2-4e28-9c64-b7edcb3d8632'\n",
    "! ../utils/download_logs.sh /nas/softechict-nas-1/rbenaglia/canomaly-data/logs/dataset-can-mnist/classes_per_task-2/logs.pyd $id_\n",
    "! ../utils/download_results.sh single_log.pyd ../storage/logs/single_log.pyd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "plot_heatmaps(df_total)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "display(HTML('''<script>function copyURI(t,e){t.preventDefault(),navigator.clipboard.writeText(e)}</script>'''))\n",
    "# for setting in ['task-il', 'class-il']:\n",
    "#     if not os.path.isdir(respath + setting): continue\n",
    "\n",
    "for dataset in [x for x in set(os.listdir(respath + setting) + (os.listdir(sec_respath + setting) if sec_respath else [])) if x not in excluded_datasets]:\n",
    "\n",
    "    print(dataset, '-', setting)\n",
    "\n",
    "    # -------------------------- READ LOG FILES INTO A SINGLE DATAFRAME ----------------------------\n",
    "    methods = None\n",
    "    for apath in [x for x in respaths if x is not None]:\n",
    "        if dataset in excluded_datasets: continue\n",
    "\n",
    "        if not os.path.isdir(apath + '%s/%s/' % (setting, dataset)): continue;\n",
    "\n",
    "        for method in os.listdir(apath + '%s/%s/' % (setting, dataset)):\n",
    "            if method.startswith('__'): continue;\n",
    "            if method in excluded_methods: continue;\n",
    "            if len(included_methods) > 0 and method not in included_methods: continue;\n",
    "            if not os.path.isfile(apath + '%s/%s/' % (setting, dataset) + method +'/logs.pyd'): continue;\n",
    "            try:\n",
    "                o = method_to_df(apath, setting, dataset, method)\n",
    "                for col in ignore_cols:\n",
    "                    if col in o.columns:\n",
    "                        o = o.drop([col], 1)\n",
    "                if methods is None:\n",
    "                    methods = o.copy()\n",
    "                else:\n",
    "                    methods = pd.concat([methods,o], axis=0, ignore_index=True, sort=False)\n",
    "            except (pd.errors.ParserError, ValueError):\n",
    "                print('Could not parse', apath, setting, dataset, method)\n",
    "    if methods is None:\n",
    "        continue\n",
    "    print('%d methods found' % len(methods))\n",
    "    methods.fillna('', inplace=True)\n",
    "    methods['conf_timestamp'] = pd.to_datetime(methods['conf_timestamp'])\n",
    "    methods = methods[~methods['method'].isin(excluded_methods)]\n",
    "    for c in methods.columns:\n",
    "        if islist(methods[c]):\n",
    "            methods[c] = methods[c].apply(str)\n",
    "\n",
    "    methods_map_time = methods.conf_timestamp > '1980'\n",
    "    # methods_map_time = methods.conf_timestamp > '2021-11-03 15:00:00'\n",
    "    if methods_map_time.sum() < 1:\n",
    "        continue\n",
    "\n",
    "    # -------------------------- DISTINGUISH COLUMNS ----------------------------\n",
    "    config_columns = [x for x in methods.columns if x.startswith('conf_')] +\\\n",
    "        ['seed', 'notes', 'csv_log', 'loss_log', 'examples_log', 'examples_full_log', 'tensorboard', 'savecheck', 'validation', 'dataset', 'ignore_other_metrics', 'balancoir', 'debug_mode','non_verbose',\n",
    "        'distributed', 'start_from', 'intensive_savecheck', 'stopafter', 'loadcheck', 'autorelaunch', 'force_compat']\n",
    "    config_columns = [x for x in config_columns if x in methods.columns]\n",
    "\n",
    "    metric_columns = [x for x in methods.columns if any(map(lambda y: x.startswith(y), \\\n",
    "                        ['accuracy_', 'accmean_', 'forward_transfer', 'backward_transfer', 'forgetting']))]\n",
    "    end = [x for x in methods.columns if x.startswith('accmean_')][-1]\n",
    "\n",
    "    methods['aic'] = methods[[x for x in methods.columns if x.startswith('accmean_')]].mean(1)\n",
    "    metric_columns.append('aic')\n",
    "\n",
    "    parameter_columns = [x for x in methods.columns if x not in config_columns and x not in metric_columns]\n",
    "\n",
    "    for c in metric_columns:\n",
    "        methods[c] = pd.to_numeric(methods[c])\n",
    "    distinct_cols = [x for x in t_distinct_cols if x in methods.columns]\n",
    "\n",
    "    # -------------------------- GROUP BY PARAMETERS ----------------------------\n",
    "    agg_dict = {end: ['count', aggf, 'std']}\n",
    "    if len(metric_columns):\n",
    "        agg_dict.update({x: ['count', aggf, 'std'] for x in metric_columns})\n",
    "    agg_dict.update({'conf_jobnum': lambda x: str(list(x.values))})\n",
    "\n",
    "    trez = methods[methods_map_time].groupby(parameter_columns).agg(agg_dict)\n",
    "    trez.columns = ['%s-%s' % (a, b) for a,b in list(trez.columns)]\n",
    "    trez = trez.reset_index().sort_values(by='%s-%s' % (end, 'aggf'), ascending=False)\n",
    "\n",
    "    # -------------------------- PREPARE OUTPUT ----------------------------\n",
    "    tmpout = trez[trez['%s-%s' %(end, 'count')] >= req_rep]\n",
    "\n",
    "    tmpout['outmetric'] = pd.Series(['0'] * len(tmpout), index=tmpout.index).str.repeat((tmpout['%s-%s' %(end, 'aggf')] < 10).astype(int).values) + tmpout['%s-%s' %(end, 'aggf')].apply(lambda x: '%.2f' % x) + ' Â± ' + tmpout['%s-%s' %(end, 'std')].apply(lambda x: '%.2f' % x)\n",
    "    tmpout = tmpout.sort_values(by='outmetric', ascending=False)\n",
    "    tmpout = tmpout.groupby(distinct_cols + ['conf_jobnum-<lambda>']).head(1)\n",
    "\n",
    "\n",
    "    for m in [end] + (metric_columns if print_other_metrics else []):\n",
    "        print('--- %s ---' % m)\n",
    "        met1 = tmpout['%s-%s' %(m, 'aggf')].apply(lambda x: '%05.2f' % x)\n",
    "        met2 = ' Â± ' + tmpout['%s-%s' %(m, 'std')].apply(lambda x: '%.2f' % x)\n",
    "        met3 = \" [\" + tmpout['%s-%s' %(m, 'count')].apply(lambda x: '%d' % x) + tmpout['conf_jobnum-<lambda>'].apply(lambda x: '] <a onclick=\"copyURI(event, \\'%s\\')\">ðŸ“‹</a>' % str(x).replace('\\'', '\\\\\\''))\n",
    "        met4 = ' (FG ' + tmpout['%s-%s' %('forgetting', 'aggf')].apply(lambda x: '%05.2f' % x) + ')'\n",
    "        tmpout['tmpmetric'] = met1  + met2 + met3 + met4\n",
    "        out = tmpout.groupby(distinct_cols)['tmpmetric'].max().unstack(-1).fillna('-')\n",
    "\n",
    "        is_max = np.equal(out.values, out.max(axis=1).values[None].T).tolist()\n",
    "\n",
    "\n",
    "        if len(out) > 0:\n",
    "            display(out.style.apply(highlight_max if m != 'forgetting' else highlight_min, axis=0))\n",
    "                \n",
    "        \n",
    "    \n",
    "            \n",
    "\n",
    "    \n",
    "        print('\\n\\n', '-' * 30, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_total.iloc[-2, -2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def series_to_args(ser):\n",
    "    global parameter_columns\n",
    "    if len(ser) == 1:\n",
    "        ser = ser.replace('', np.nan)[parameter_columns].dropna(1, 'all').iloc[0]\n",
    "    s = ser.to_dict()\n",
    "    del s['method']\n",
    "    bits = []\n",
    "    for x, y in s.items():\n",
    "        if type(y) != float or np.isnan(y):\n",
    "            if type(y) == str and y[0] == '[' and y[-1] == ']':\n",
    "                bits.append(f'--{x} {y[1:-1].replace(\",\", \" \")}')\n",
    "            else:\n",
    "                bits.append(f'--{x}={(str(y) if not str(y).endswith(\".0\") else str(y)[:-2])}')\n",
    "\n",
    "                \n",
    "    return ' '.join(bits)\n",
    "    \n",
    "def gridmap(methods, jobs1, jobs2=None, titles=None, delta=False):\n",
    "    job1l = []\n",
    "    \n",
    "    ntasks = sorted([int(x.split('task')[-1]) for x in methods.columns if x.startswith('accmean_task')])[-1]\n",
    "    fsize=(ntasks*0.6, ntasks*0.6)\n",
    "\n",
    "    for job in jobs1:\n",
    "        arrayv = []\n",
    "        mydata = methods[methods.conf_jobnum == job][[x for x in metric_columns if 'accuracy_' in x]].iloc[0]\n",
    "        for i in range(1, ntasks+1):\n",
    "            arrayv.append([])\n",
    "            for j in range(1, i+1):\n",
    "                arrayv[-1].append(mydata[f'accuracy_{j}_task{i}'] - (0 if not delta else 100 / i))\n",
    "            avg = np.mean(arrayv[-1])\n",
    "            for j in range(ntasks-i):\n",
    "                arrayv[-1].append(np.nan)\n",
    "            arrayv[-1].append(avg)\n",
    "        job1l.append(np.array(arrayv))\n",
    "    plt.figure(figsize=fsize)\n",
    "    sns.heatmap(np.stack(job1l).mean(0), annot=True, fmt='.1f')\n",
    "    if titles is not None:\n",
    "        plt.title(titles[0])\n",
    "    else:\n",
    "        plt.title(methods[methods.conf_jobnum ==job].method.iloc[0])\n",
    "    \n",
    "    if jobs2 is not None:\n",
    "        job2l = []\n",
    "        for job in jobs2:\n",
    "            arrayv = []\n",
    "            mydata = methods[methods.conf_jobnum == job][[x for x in metric_columns if 'accuracy_' in x]].iloc[0]\n",
    "            for i in range(1, ntasks+1):\n",
    "                arrayv.append([])\n",
    "                for j in range(1, i+1):\n",
    "                    arrayv[-1].append(mydata[f'accuracy_{j}_task{i}'] - (0 if not delta else 100 / i))\n",
    "                avg = np.mean(arrayv[-1])\n",
    "                for j in range(ntasks-i):\n",
    "                    arrayv[-1].append(np.nan)\n",
    "                arrayv[-1].append(avg)\n",
    "            job2l.append(np.array(arrayv))\n",
    "        plt.figure(figsize=fsize)\n",
    "        sns.heatmap(np.stack(job2l).mean(0), annot=True, fmt='.1f')\n",
    "        if titles is not None:\n",
    "            plt.title(titles[1])\n",
    "        else:\n",
    "            plt.title(methods[methods.conf_jobnum ==job].method.iloc[0])\n",
    "        \n",
    "        conf = np.stack(job1l).mean(0) - np.stack(job2l).mean(0)\n",
    "        \n",
    "        plt.figure(figsize=fsize)\n",
    "        sns.heatmap(conf, annot=True, cmap='bwr', vmin=-np.nanmax(abs(conf)), vmax=np.nanmax(abs(conf)))\n",
    "        if titles is not None:\n",
    "            plt.title(titles[0] + ' - ' + titles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridmap(methods, ['3ea70747-8697-4e76-bc4f-319c6fa312b5', '89f4c672-f16a-4b08-a509-a94c82a6ebbd', '31d4cc8c-a727-4665-8643-2212eb519aa6', '26ea2435-d2d4-4988-b37e-d95934301d2f', 'a0626b3b-b34f-4353-8453-4abd769c130c', '9ae21984-b77f-4d12-be42-c4929c4d4125', '1cc0e452-8112-47ac-b939-f9ad5ae5edc6', '86d1bc60-d263-46be-b90f-ff9d9a30173a', '606653f0-b606-426c-9e8f-7770e8dad36a', '6ef54be8-cedf-40d7-a7c7-cda396b45d1c', '918ff2d4-b0d1-456d-96a1-e283e7838827'],\n",
    "# ['57976485-efee-4aae-95e2-4a2ac112ac3e', 'ff4eb4f9-0e6b-4336-bd66-057a02d2f663', '590d6225-04f7-4ba6-b8c7-0182262ccdf5', '78b3b890-bcc0-4fa6-bc7a-2569f12cee23', '0c875241-6230-41d1-9875-09a2c4c586c0', 'f608773d-99e2-49c9-b1d2-6857966d6977', 'f9a60776-ec0b-47fd-8895-27743c5c9911', '74b41430-89b3-4c1c-a058-9e01bb05480a', '45679ee3-d077-4e5a-83cc-77d4670d27ea', '1e7573db-8821-4cd7-b243-3f4fe9d31553', 'ad3fa7bc-8a5a-4fa6-a2b1-5d3072597e02'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a4aedbe07a0908b7e4bcbb5b24e255736426370940ab7ed4cb0f0eb179b2097"
  },
  "kernelspec": {
   "display_name": "canomaly",
   "language": "python",
   "name": "canomaly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
