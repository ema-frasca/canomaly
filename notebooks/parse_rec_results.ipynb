{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from ast import literal_eval\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "display(HTML(\"<style>\"\n",
    "    + \"#notebook { padding-top:0px; } \" \"\"\n",
    "    + \".container { width:100%; } \"\n",
    "    + \".end_space { min-height:0px; } \"\n",
    "    + \"</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logs_path = 'C:\\\\Users\\\\emace\\\\AImageLab\\\\SRV-Continual\\\\results\\\\canomaly\\\\results'\n",
    "logs_path = '../storage/results/dataset-can-cifar10'\n",
    "logs_path = '../storage/results/dataset-rec-fmnist'\n",
    "logs_path = '/nas/softechict-nas-2/efrascaroli/canomaly-data/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp_dict = {}\n",
    "environments = []\n",
    "for log_file in glob(logs_path + '/**/*.pyd', recursive=True):\n",
    "    print(log_file)\n",
    "    with open(log_file, 'r') as f:\n",
    "        props_list = [prop.split('-', 1) for prop in log_file.replace(logs_path, '').split('\\\\')[1:-1]]\n",
    "        props = {prop[0]: prop[1] for prop in props_list}\n",
    "        exps = []\n",
    "        for i, line in enumerate(f.readlines()):\n",
    "            try:\n",
    "                exps.append(literal_eval(line.replace('nan', 'None')))\n",
    "            except:\n",
    "                print(f'Unparsed line {i}:\\n\\t{exps[:-1]}\\n-->\\t{line}')\n",
    "\n",
    "        environments.append({'env': props, 'exps': exps})\n",
    "\n",
    "        exps = {exp['id']: exp for exp in exps}\n",
    "        exp_dict = {**exp_dict, **exps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_exp_info(exp: dict):\n",
    "    print({k: exp[k] for k in exp if k not in ['auc_final', 'auc_average', 'auc_per_task', 'conf_matrix_per_task']})\n",
    "\n",
    "# print_exp_info(environments[0]['exps'][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_df_form_dict(d: dict):\n",
    "    return pd.DataFrame.from_dict(d, orient='index')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_aucs_per_task(task_aucs: pd.DataFrame, annotations: pd.DataFrame=None):\n",
    "    fig, ax = plt.subplots(figsize=(10, len(task_aucs)+1))\n",
    "    sns.heatmap(task_aucs,\n",
    "                annot=annotations if annotations is not None else True,\n",
    "                fmt='' if annotations is not None else '.2g',\n",
    "                ax=ax, cmap='Reds', cbar=False)\n",
    "    plt.ylabel('Task')\n",
    "    plt.xlabel('Class')\n",
    "    plt.title('Auc per task and class')\n",
    "    plt.show()\n",
    "\n",
    "# show_aucs_per_task(pd.DataFrame.from_dict(environments[0]['exps'][0]['auc_per_task'], orient='index'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_conf_matrix(cmatrix: pd.DataFrame):\n",
    "    fig, ax = plt.subplots(figsize=(10, len(cmatrix)+1))\n",
    "    sns.heatmap(data=cmatrix, ax=ax,annot=True, cbar=False, cmap='Reds')\n",
    "    plt.ylabel('Task')\n",
    "    plt.xlabel('Class')\n",
    "    plt.title('Reconstruction error per task and class')\n",
    "    plt.show()\n",
    "\n",
    "# show_conf_matrix(pd.DataFrame.from_dict(environments[0]['exps'][0]['conf_matrix_per_task'], orient='index'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def disclosure_experiment(exp: dict, info=True, aucs=True, cmatrix=True):\n",
    "    ## print metrics of experiments\n",
    "    if info:\n",
    "        print_exp_info(exp)\n",
    "    if aucs:\n",
    "        print(f'final {exp[\"auc_final\"]} average {exp[\"auc_average\"]}')\n",
    "        show_aucs_per_task(get_df_form_dict(exp['auc_per_task']))\n",
    "\n",
    "    if cmatrix:\n",
    "        show_conf_matrix(get_df_form_dict(exp['conf_matrix_per_task']))\n",
    "\n",
    "# disclosure_experiment(exp_dict['a8b7df79-7031-4f95-ba9c-aaae316ced5e'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "useless_cols = ['seed', 'logs', 'auc_per_task', 'conf_matrix_per_task']\n",
    "# cols without duplicates\n",
    "model_cols = {\n",
    "    'common_pre': ['n_epochs', 'batch_size', 'optim', 'lr', 'model'],\n",
    "    'ae': ['latent_space'],\n",
    "    'dae': ['noise_mean', 'noise_std'],\n",
    "    'sae': ['sparse_weight', 'norm_order'],\n",
    "    'vae': ['kl_weight', 'beta_kl', 'forward_sample'],\n",
    "    'rec-vae': ['kl_weight', 'forward_sample'],\n",
    "    'common_post': ['joint', 'splits']\n",
    "}\n",
    "result_cols = ['rec_mean']\n",
    "\n",
    "group_cols = [col for cols in model_cols for col in model_cols[cols]]+['timestamp']\n",
    "order_cols = group_cols + ['id'] + result_cols\n",
    "\n",
    "methods_list = []\n",
    "def method_id(df: pd.DataFrame):\n",
    "    idx = len(methods_list)\n",
    "    methods_list.append({'exps': list(df)})\n",
    "    return idx\n",
    "\n",
    "for env in environments:\n",
    "    env_cols = [prop for prop in env['env']]\n",
    "\n",
    "    results = pd.DataFrame.from_records(env['exps'])\n",
    "    tot_cols = results.columns.tolist()\n",
    "    exclude_cols = list(set(col for col in useless_cols + env_cols if col in tot_cols))\n",
    "    orderby_cols = list(set(col for col in order_cols if col in tot_cols))\n",
    "    groupby_cols = list(set(col for col in group_cols if col in tot_cols))\n",
    "    unknown_cols = list(set(col for col in tot_cols if col not in exclude_cols + orderby_cols))\n",
    "    env['unknown'] = unknown_cols\n",
    "    results = results[orderby_cols]\n",
    "    results['runs'] = 1\n",
    "    results = results.groupby(groupby_cols, dropna=False).agg(\n",
    "        {'runs': 'count', 'id': method_id,\n",
    "         # **{res: ['mean', 'std'] for res in result_cols}\n",
    "         }\n",
    "    )\n",
    "    for index, res in results.iterrows():\n",
    "        id_met = int(res['id'])\n",
    "        methods_list[id_met]['props'] = {name: index[i] for i, name in enumerate(results.index.names)}\n",
    "        methods_list[id_met]['env'] = env['env']\n",
    "        # methods_list[id_met]['results'] = {res_col: {'mean': res[(res_col, 'mean')], 'std': res[(res_col, 'std')]} for res_col in result_cols}\n",
    "\n",
    "    env['results'] = results\n",
    "\n",
    "sort = False\n",
    "sort_col = ('auc_final', 'mean')\n",
    "for env in environments:\n",
    "    print('ENV INFO - ' + str(env['env']))\n",
    "    if len(env['unknown']):\n",
    "        print('-- unknown props: ' + str(env['unknown']))\n",
    "    display(env['results'].sort_values(sort_col, ascending=False) if sort else env['results'])\n",
    "    print('-'*100 + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def agg_annots(df: pd.DataFrame):\n",
    "    m, s = list(df)\n",
    "    if np.isnan(m):\n",
    "        return str(m)\n",
    "    return f'{m:.2f}Â±{s:.2f}'\n",
    "\n",
    "def disclosure_method(idx: int, info=False, aucs=False, cmatrix=False):\n",
    "    method = methods_list[idx]\n",
    "    print(f'METHOD {idx}')\n",
    "    print('ENV - ' + str(method['env']))\n",
    "    print('PROPS - ' + str(method['props']))\n",
    "    # print('RESULTS - ' + str(method['results']))\n",
    "    print(f'EXPERIMENTS ({len(method[\"exps\"])}):')\n",
    "    exps = [exp_dict[eid] for eid in method['exps']]\n",
    "    # for exp in exps:\n",
    "    #     print(f'-- {exp[\"id\"]}: \\tauc-final {exp[\"auc_final\"]:.4f} \\tauc-average {exp[\"auc_average\"]:.4f} \\t {\"logs\" if exp[\"logs\"] == True else \"\"}')\n",
    "\n",
    "    if info:\n",
    "        print('INFO')\n",
    "        for exp in exps:\n",
    "            print_exp_info(exp)\n",
    "    if aucs:\n",
    "        exp_aucs = [get_df_form_dict(exp['auc_per_task']) for exp in exps]\n",
    "        df = pd.concat(exp_aucs)\n",
    "        means = df.groupby(level=0).mean()\n",
    "        stds = df.groupby(level=0).std()\n",
    "        annots = pd.concat([means, stds]).groupby(level=0).agg(agg_annots)\n",
    "        # display(annots)\n",
    "        show_aucs_per_task(means, annotations=annots)\n",
    "\n",
    "    if cmatrix:\n",
    "        exp_aucs = [get_df_form_dict(exp['conf_matrix_per_task']) for exp in exps]\n",
    "        df = pd.concat(exp_aucs)\n",
    "        means = df.groupby(level=0).mean()\n",
    "        stds = df.groupby(level=0).std()\n",
    "        annots = pd.concat([means, stds]).groupby(level=0).agg(agg_annots)\n",
    "        # display(annots)\n",
    "        show_conf_matrix(means)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "disclosure_method(2, info=False, aucs=False, cmatrix=True)\n",
    "disclosure_method(1, info=False, aucs=False, cmatrix=True)\n",
    "# disclosure_method(2, info=False, aucs=True, cmatrix=True)\n",
    "# disclosure_method(30, info=False, aucs=True, cmatrix=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}